---
title: "PredictionCourseProject Hamoud Alshammari"
output: html_document
---

In this project I am goining to provide a ML predection algrithm to predict the behavior of six people in five different exercises that they were asked to do correctly. I will follow very common steps to prepare the data fist as follows:
1. read training data and divide it into training by 60% and testing by 40%.

```{r}
setwd("~/Desktop/Coursera/WorkDirectory/pml-project")
TrainingData= read.csv("pml-training.csv", header = TRUE)
TestingData= read.csv("pml-testing.csv", header = TRUE)
```

Using propoer libraries:
```{r}
library(caret)
library(rpart)
library(rpart.plot)
library(RColorBrewer)
library(rattle)
library(randomForest)

set.seed(12345)
inTrain <- createDataPartition(y=TrainingData$classe, p=0.6, list=FALSE)

myTraining <- TrainingData[inTrain, ]
myTesting <- TrainingData[-inTrain, ]
dim(myTraining)
dim(myTesting)
```

Then I need to delete the columns that contain NA values more than 50% to make the values more relaible to be used.
```{r, echo=FALSE}
myDataNZV<- nearZeroVar(myTraining, saveMetrics = TRUE)

myNZVvars <- names(myTraining) %in% c("new_window", "kurtosis_roll_belt", "kurtosis_picth_belt",
                                      "kurtosis_yaw_belt", "skewness_roll_belt", "skewness_roll_belt.1", "skewness_yaw_belt",
                                      "max_yaw_belt", "min_yaw_belt", "amplitude_yaw_belt", "avg_roll_arm", "stddev_roll_arm",
                                      "var_roll_arm", "avg_pitch_arm", "stddev_pitch_arm", "var_pitch_arm", "avg_yaw_arm",
                                      "stddev_yaw_arm", "var_yaw_arm", "kurtosis_roll_arm", "kurtosis_picth_arm",
                                      "kurtosis_yaw_arm", "skewness_roll_arm", "skewness_pitch_arm", "skewness_yaw_arm",
                                      "max_roll_arm", "min_roll_arm", "min_pitch_arm", "amplitude_roll_arm", "amplitude_pitch_arm",
                                      "kurtosis_roll_dumbbell", "kurtosis_picth_dumbbell", "kurtosis_yaw_dumbbell", "skewness_roll_dumbbell",
                                      "skewness_pitch_dumbbell", "skewness_yaw_dumbbell", "max_yaw_dumbbell", "min_yaw_dumbbell",
                                      "amplitude_yaw_dumbbell", "kurtosis_roll_forearm", "kurtosis_picth_forearm", "kurtosis_yaw_forearm",
                                      "skewness_roll_forearm", "skewness_pitch_forearm", "skewness_yaw_forearm", "max_roll_forearm",
                                      "max_yaw_forearm", "min_roll_forearm", "min_yaw_forearm", "amplitude_roll_forearm",
                                      "amplitude_yaw_forearm", "avg_roll_forearm", "stddev_roll_forearm", "var_roll_forearm",
                                      "avg_pitch_forearm", "stddev_pitch_forearm", "var_pitch_forearm", "avg_yaw_forearm",
                                      "stddev_yaw_forearm", "var_yaw_forearm")

myTraining <- myTraining[!myNZVvars]
```

The Data size with NA values is:
```{r, echo=FALSE}
dim(myTraining)
myTraining <- myTraining[c(-1)]
trainingV3 <- myTraining
```

The Data size without NA values is:
```{r, echo=FALSE}
for(i in 1:length(myTraining))  #for every column in the training dataset
{ 
  if( sum( is.na( myTraining[, i] ) ) /nrow(myTraining) >= .6 )  #if n?? NAs > 60% of total observations
  { 
    for(j in 1:length(trainingV3)) 
    {
      if( length( grep(names(myTraining[i]), names(trainingV3)[j]) ) == 1)  #if the columns are the same:
      { 
        trainingV3 <- trainingV3[ , -j] #Remove that column
      }   
    } 
  }
}

dim(trainingV3)

myTraining <- trainingV3
rm(trainingV3)
```

test the 40% testing data to make sure the predection goes fine, and preparing testing data.The sizes is as follows:
```{r, echo=FALSE}
clean1 <- colnames(myTraining)
clean2 <- colnames(myTraining[, -58]) #already with classe column removed
myTesting <- myTesting[clean1]

testing <- TestingData[clean2]

dim(myTraining)
dim(myTesting)
dim(testing)

for (i in 1:length(testing) ) 
{
  for(j in 1:length(myTraining)) 
  {
    if( length( grep(names(myTraining[i]), names(testing)[j]) ) ==1)  
    {
      class(testing[j]) <- class(myTraining[i])
    }      
  }      
}

testing <- rbind(myTraining[2, -58] , testing) #note row 2 does not mean anything, this will be removed right.. now:
testing <- testing[-1,]
dim(testing)
```


The Decision Tree ml goes as:
```{r, echo=FALSE}
modFitA1 <- rpart(classe ~ ., data=myTraining, method="class")
fancyRpartPlot(modFitA1)
predictionsA1 <- predict(modFitA1, myTesting, type = "class")
confusionMatrix(predictionsA1, myTesting$classe)
```

# Random Forests
```{r, echo=FALSE}
modFitB1 <- randomForest(classe ~. , data=myTraining)
predictionsB1 <- predict(modFitB1, myTesting, type = "class")
confusionMatrix(predictionsB1, myTesting$classe)
```

Random Forests gaves better Results than TREE.

Final answer for 20 predictions.
```{r, echo=FALSE}
predictionsB2 <- predict(modFitB1, testing, type = "class")

pml_write_files = function(x){
  n = length(x)
  for(i in 1:n){
    filename = paste0("problem_id_",i,".txt")
    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
  }
}

pml_write_files(predictionsB2)
```


